{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\ann\\venv\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in d:\\ann\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in d:\\ann\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\ann\\venv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in d:\\ann\\venv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in d:\\ann\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\"hello welcome, how are you ?\n",
    "today we are going to learn the tokenization of strings and words.\n",
    "we will use the NLTK library to do this.\n",
    "we will also use the wordnet lemm\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting sentences into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yuvra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent=sent_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello welcome, how are you ?\n",
      "today we are going to learn the tokenization of strings and words.\n",
      "we will use the NLTK library to do this.\n",
      "we will also use the wordnet lemm\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized_sent:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'today',\n",
       " 'we',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'tokenization',\n",
       " 'of',\n",
       " 'strings',\n",
       " 'and',\n",
       " 'words',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'use',\n",
       " 'the',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'also',\n",
       " 'use',\n",
       " 'the',\n",
       " 'wordnet',\n",
       " 'lemm']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converting sentence and word into tokens at the same place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'welcome', ',', 'how', 'are', 'you', '?']\n",
      "['today', 'we', 'are', 'going', 'to', 'learn', 'the', 'tokenization', 'of', 'strings', 'and', 'words', '.']\n",
      "['we', 'will', 'use', 'the', 'NLTK', 'library', 'to', 'do', 'this', '.']\n",
      "['we', 'will', 'also', 'use', 'the', 'wordnet', 'lemm']\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized_sent:\n",
    "    print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wordpunct tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'today',\n",
       " 'we',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'tokenization',\n",
       " 'of',\n",
       " 'strings',\n",
       " 'and',\n",
       " 'words',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'use',\n",
       " 'the',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'also',\n",
       " 'use',\n",
       " 'the',\n",
       " 'wordnet',\n",
       " 'lemm']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree bank tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'welcome',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'today',\n",
       " 'we',\n",
       " 'are',\n",
       " 'going',\n",
       " 'to',\n",
       " 'learn',\n",
       " 'the',\n",
       " 'tokenization',\n",
       " 'of',\n",
       " 'strings',\n",
       " 'and',\n",
       " 'words.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'use',\n",
       " 'the',\n",
       " 'NLTK',\n",
       " 'library',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'also',\n",
       " 'use',\n",
       " 'the',\n",
       " 'wordnet',\n",
       " 'lemm']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
